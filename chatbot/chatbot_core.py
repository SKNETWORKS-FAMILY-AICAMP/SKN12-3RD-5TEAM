import numpy as np
import config
from rag.vector_store import load_category_vector_db, load_vector_db_by_path
from rag.embedder import load_embedder
from llm.category_classifier import classify_category_with_llm
from llm.router import get_llm_by_category
from llm.responder import build_rag_chain
from llm.chatbot_llm import chatbot_response
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from sklearn.metrics.pairwise import cosine_similarity

# ---- ì´ˆê¸°í™” ----
embedder = load_embedder()
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
path, _ = config.VECTOR_DB_PATHS.get("treatment", config.VECTOR_DB_PATHS["default"])
faiss_db = FAISS.load_local(
    folder_path=path,
    embeddings=embedding_model,
    allow_dangerous_deserialization=True  # ì§ì ‘ ë§Œë“  ê²½ìš°ì—ë§Œ!
)

category_texts, category_categories, category_embeddings = load_category_vector_db()

# ğŸ’¡ category_embeddings (N, D), normalize
def normalize(v):
    return v / (np.linalg.norm(v, axis=-1, keepdims=True) + 1e-8)
category_embeddings_norm = normalize(category_embeddings.numpy())

def run_chatbot_pipeline(user_input: str, session_id: str = "default") -> str:
    # 1. ì…ë ¥ ì„ë² ë”© + ì •ê·œí™”
    input_emb = embedder.encode([user_input])[0]  # (D,)
    input_emb_norm = input_emb / (np.linalg.norm(input_emb) + 1e-8)
    
    # 2. ì½”ì‚¬ì¸ ìœ ì‚¬ë„(0~1) top-k
    sims = np.dot(category_embeddings_norm, input_emb_norm)  # (N,)
    top_k = 3
    top_idx = np.argsort(sims)[-top_k:][::-1]
    top_sims = sims[top_idx]
    max_sim = top_sims[0]
    
    if max_sim < 0.5:
        # context ì—†ì´ ë°”ë¡œ ì±—ë´‡ LLMì— ì „ë‹¬
        return chatbot_response(user_input, "", session_id=session_id)
    
    # 3. context í™•ë³´, ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ LLM
    retrieved_examples = [
        (category_texts[i], category_categories[i], float(top_sims[j]))
        for j, i in enumerate(top_idx)
        if top_sims[j] >= 0.5
    ]
    # ë§Œì•½ ì•„ë¬´ê²ƒë„ ì—†ìœ¼ë©´, context ì—†ì´ ë°”ë¡œ ì‘ë‹µ
    if not retrieved_examples:
        return chatbot_response(user_input, "", session_id=session_id)

    predicted_category = classify_category_with_llm(user_input, retrieved_examples)

    # 4. ì¹´í…Œê³ ë¦¬ë³„ ë²¡í„° DBì—ì„œ ë¬¸ì„œ ì¬ê²€ìƒ‰ (context ì¶”ì¶œ)
    if predicted_category == "treatment":
        results = faiss_db.similarity_search(user_input, k=3)
        context = "\n\n".join([doc.page_content for doc in results])
    else:
        index_file, chunks_file = config.VECTOR_DB_PATHS.get(predicted_category, config.VECTOR_DB_PATHS["default"])
        context_docs, context_index = load_vector_db_by_path(index_file, chunks_file)
        doc_emb = embedder.encode([user_input])[0]
        doc_emb_norm = doc_emb / (np.linalg.norm(doc_emb) + 1e-8)
        context_docs_norm = np.array([c / (np.linalg.norm(c) + 1e-8) for c in context_index.reconstruct_n(0, context_index.ntotal)])
        sims2 = np.dot(context_docs_norm, doc_emb_norm)

        # ìœ ì‚¬ë„ 0.5 ì´ìƒë§Œ ì¶”ì¶œ (ìµœëŒ€ 3ê°œ)
        selected_idx = [i for i in np.argsort(sims2)[::-1] if sims2[i] >= 0.5][:3]

        if selected_idx:
            top_docs = [context_docs[i] for i in selected_idx]
            context = "\n".join(top_docs)
        else:
            context = ""   # ìœ ì‚¬í•œ ë¬¸ì¥ì´ ì—†ìœ¼ë©´ context ì—†ì´!

    # 5. ì¹´í…Œê³ ë¦¬ë³„ ì „ë¬¸ LLM â†’ 1ì°¨ ë‹µë³€
    category_llm = get_llm_by_category(predicted_category)
    rag_chain = build_rag_chain(category_llm)
    expert_response = rag_chain.invoke({"question": user_input, "context": context})

    # 6. ìµœì¢… ì±—ë´‡ LLM
    return chatbot_response(user_input, expert_response, session_id=session_id)
